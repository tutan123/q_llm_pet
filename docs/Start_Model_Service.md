# ğŸš€ æ¨¡å‹æœåŠ¡å¿«é€Ÿå¯åŠ¨æŒ‡å—

æœ¬æ–‡æ¡£æä¾›å¯åŠ¨å¾®è°ƒåçš„ **pet-model** æœåŠ¡çš„æ ¸å¿ƒæ­¥éª¤ã€‚

## 1. ç¯å¢ƒå‡†å¤‡ (WSL2 / Linux)

é¦–å…ˆè¿›å…¥ Conda è™šæ‹Ÿç¯å¢ƒï¼š

```bash
conda activate infer_vllm
```

## 2. å¯åŠ¨ vLLM æœåŠ¡

æ‰§è¡Œä»¥ä¸‹å‘½ä»¤å¯åŠ¨æ¨¡å‹æœåŠ¡ï¼ˆå·²æ ¹æ®æ‚¨çš„æ˜¾å­˜é…ç½®ä¼˜åŒ–ï¼‰ï¼š

```bash
vllm serve EdgeAI/outputs/official_lora_pet_merged_20251228_210849/ \
  --served-model-name pet-model \
  --trust-remote-code \
  --dtype bfloat16 \
  --gpu-memory-utilization 0.3 \
  --max-model-len 1024
```

### å‚æ•°ç®€è¦è¯´æ˜ï¼š
*   `--served-model-name`: å®šä¹‰æ¨¡å‹åç§°ä¸º `pet-model`ï¼ˆéœ€åœ¨å‰ç«¯ Settings ä¸­åŒ¹é…ï¼‰ã€‚
*   `--gpu-memory-utilization`: è®¾ç½®ä¸º `0.3`ï¼ˆå ç”¨ 30% æ˜¾å­˜ï¼‰ï¼Œé€‚åˆåœ¨è¿è¡Œå…¶ä»–ç¨‹åºæ—¶èŠ‚çœèµ„æºã€‚
*   `--max-model-len`: é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦ä¸º `1024`ï¼Œæé«˜å“åº”é€Ÿåº¦ã€‚

## 3. å‰ç«¯é…ç½®è¿æ¥

å¯åŠ¨æœåŠ¡åï¼Œåœ¨ç½‘é¡µçš„ **Configuration** é¢æ¿è¿›è¡Œå¦‚ä¸‹é…ç½®ï¼š

*   **Provider**: é€‰æ‹© `Gemma` æˆ– `OpenAI`ã€‚
*   **API Endpoint**: `http://localhost:8000/v1/completions` (Gemma) æˆ– `http://localhost:8000/v1` (OpenAI)ã€‚
*   **Model Name**: `pet-model`ã€‚

---
*Last Updated: 2026-01-01*

