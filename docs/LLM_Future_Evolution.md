# 🐧 Q-Penguin: 大模型非语言指令交互演进课题

## 1. 核心课题背景
当前的 Q-Penguin 架构已成功集成了基于文本指令的行为决策（Function Calling）。然而，为了让数字生命更加“真实”，它不应仅仅是一个“对话框驱动”的机器人，而应具备对物理世界（虚拟环境）的感知能力，并能基于这些感知产生自发的行为。

## 2. 演进方向：非语言指令场景

### 2.1 环境感知与情感映射
大模型可以不再直接接收用户的“命令”，而是接收当前的“状态快照”。
*   **温度感知**：空调开启、室温降低。模型接收 `{"temp": 16, "status": "aircon_on"}`，自动输出 `{"action": "SHIVER", "expression": "COLD"}`。
*   **光照交互**：当用户将企鹅拖动到虚拟阳光下。模型接收 `{"light_intensity": 0.9, "area": "sunny"}`，自动输出 `{"action": "SQUINT", "view": "LOOK_UP"}`。
*   **时段触发**：深夜。模型接收 `{"time": "23:30"}`，自动触发 `{"action": "YAWN", "state": "SLEEPY"}`。

### 2.2 物理交互的“泛化解释”
目前点击和拖拽是硬编码的，未来可以通过大模型进行语义化解释。
*   **恶意拖拽 vs. 温柔抚摸**：通过分析鼠标移动的轨迹（抖动频率、速度）。
    *   高速无规律移动 -> 模型解释为“受惊”，输出挣扎动作。
    *   缓慢圆周运动 -> 模型解释为“抚摸”，输出享受动作。
*   **点击位置差异**：点击头部 vs. 点击脚部。大模型根据点击坐标生成不同的社交反馈。

### 2.3 自主目标设定 (Autonomous Goal Setting)
引入“渴望”或“需求”系统（如饥饿值、孤独值）。
*   当 `loneliness > 0.8` 且用户长时间未输入。模型自发输出 `{"action": "WAVE", "text": "嘿，理理我嘛..."}`。
*   这要求模型具备长时间跨度的上下文记忆，而不仅仅是单次交互。

## 3. 技术演进路径

| 阶段 | 目标 | 核心技术 |
| :--- | :--- | :--- |
| **阶段 1 (当前)** | 动作微调与 BT 集成 | 文本 -> 动作映射, 确定性 BT 分支 |
| **阶段 2 (近期)** | 环境参数注入 | 将环境 JSON 拼接到 Prompt, 增加环境 Condition |
| **阶段 3 (中期)** | 多模态输入解析 | 轨迹分析算法 -> 意图标签 -> LLM 决策 |
| **阶段 4 (远期)** | 自主意识系统 | 长期记忆 (RAG), 需求驱动的自发 Tick 调用 |

## 4. 合理化建议 (非画蛇添足)
*   **分层决策**：简单的物理反馈（如提起瞬间的位移）保持在 BT 的 C++ / TS 层，只有需要“决定态度”的时候才调用 LLM。
*   **异步推理缓存**：针对环境变化（如变冷），LLM 推理一次后结果可保持一段时间，无需高频调用。
*   **小模型本地化**：FunctionGemma 极其适合此类任务，因为它不需要庞大的世界知识，只需精通“状态 -> 动作”的映射。

---
*文档编制：AI Assistant*
*日期：2025年12月30日*

